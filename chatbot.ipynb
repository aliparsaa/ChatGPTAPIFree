{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlw1+fI7RoGFhihnAdVlbL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aliparsaa/ChatGPTAPIFree/blob/main/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fastapi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWuEJcnfa2WQ",
        "outputId": "c90f9a90-ce4f-4560-9f8d-6b110d2a6332"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.8-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting starlette<0.46.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.10.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.2)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.46.0,>=0.40.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Downloading fastapi-0.115.8-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: starlette, fastapi\n",
            "Successfully installed fastapi-0.115.8 starlette-0.45.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install GPT4All"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uytsirn5bNt-",
        "outputId": "3a977158-2c38-4564-cb55-a8c8e5b84026"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting GPT4All\n",
            "  Downloading gpt4all-2.8.2-py3-none-manylinux1_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from GPT4All) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from GPT4All) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->GPT4All) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->GPT4All) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->GPT4All) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->GPT4All) (2025.1.31)\n",
            "Downloading gpt4all-2.8.2-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: GPT4All\n",
            "Successfully installed GPT4All-2.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-IQ3_M.gguf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2IN_O1pdJLP",
        "outputId": "6019eb60-2edb-4594-8ca4-975814cfbf59"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-24 19:35:00--  https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-IQ3_M.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 3.169.137.111, 3.169.137.5, 3.169.137.119, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.169.137.111|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/83/6a/836a2383aaf9396df7e51349b13c7700c207710455ae1353ae38fbaa0e4c9cfa/c80cc062a721c267ec50fee83fe6b55d36fc7abe708392dbe22e16fbd42687e8?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Llama-3.2-1B-Instruct-IQ3_M.gguf%3B+filename%3D%22Llama-3.2-1B-Instruct-IQ3_M.gguf%22%3B&Expires=1740429300&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDQyOTMwMH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzgzLzZhLzgzNmEyMzgzYWFmOTM5NmRmN2U1MTM0OWIxM2M3NzAwYzIwNzcxMDQ1NWFlMTM1M2FlMzhmYmFhMGU0YzljZmEvYzgwY2MwNjJhNzIxYzI2N2VjNTBmZWU4M2ZlNmI1NWQzNmZjN2FiZTcwODM5MmRiZTIyZTE2ZmJkNDI2ODdlOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=vdr%7EyHtt1o4GVF66WTCZ8%7EIY9-BOgA32Nc8Wdv%7EUhrHN0Tb4pL478aFs97saViXSVoZXOl9PSW31PazStUua1mes-caYAE7IERX8loPeA3vaF9DeWTwcoYQX4OxLphAIba8UhC9G%7EOnSGWMarjj0Y2--6-vuNt5TivHG8mwlKohoHnae91HKo3D2xXgUTGrgv9rcKrDfZyEO2JLxjssYe3dbVfyxd2p2SPkzpxugWzKVs7xKX-7FaDEgT5cRCsZfKVvWN2-6E9HyJJ4L6PLX85bvcUZuc4o0PE0tMv3Ym%7EXxAX8V9cfgfPhpyvbtkFx%7ElH2EGt2L6IABZEkScoinkA__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-02-24 19:35:00--  https://cdn-lfs-us-1.hf.co/repos/83/6a/836a2383aaf9396df7e51349b13c7700c207710455ae1353ae38fbaa0e4c9cfa/c80cc062a721c267ec50fee83fe6b55d36fc7abe708392dbe22e16fbd42687e8?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Llama-3.2-1B-Instruct-IQ3_M.gguf%3B+filename%3D%22Llama-3.2-1B-Instruct-IQ3_M.gguf%22%3B&Expires=1740429300&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDQyOTMwMH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzgzLzZhLzgzNmEyMzgzYWFmOTM5NmRmN2U1MTM0OWIxM2M3NzAwYzIwNzcxMDQ1NWFlMTM1M2FlMzhmYmFhMGU0YzljZmEvYzgwY2MwNjJhNzIxYzI2N2VjNTBmZWU4M2ZlNmI1NWQzNmZjN2FiZTcwODM5MmRiZTIyZTE2ZmJkNDI2ODdlOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=vdr%7EyHtt1o4GVF66WTCZ8%7EIY9-BOgA32Nc8Wdv%7EUhrHN0Tb4pL478aFs97saViXSVoZXOl9PSW31PazStUua1mes-caYAE7IERX8loPeA3vaF9DeWTwcoYQX4OxLphAIba8UhC9G%7EOnSGWMarjj0Y2--6-vuNt5TivHG8mwlKohoHnae91HKo3D2xXgUTGrgv9rcKrDfZyEO2JLxjssYe3dbVfyxd2p2SPkzpxugWzKVs7xKX-7FaDEgT5cRCsZfKVvWN2-6E9HyJJ4L6PLX85bvcUZuc4o0PE0tMv3Ym%7EXxAX8V9cfgfPhpyvbtkFx%7ElH2EGt2L6IABZEkScoinkA__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 3.169.36.120, 3.169.36.128, 3.169.36.38, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|3.169.36.120|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 657289344 (627M) [binary/octet-stream]\n",
            "Saving to: ‘Llama-3.2-1B-Instruct-IQ3_M.gguf’\n",
            "\n",
            "Llama-3.2-1B-Instru 100%[===================>] 626.84M  41.1MB/s    in 15s     \n",
            "\n",
            "2025-02-24 19:35:16 (40.9 MB/s) - ‘Llama-3.2-1B-Instruct-IQ3_M.gguf’ saved [657289344/657289344]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "8KDhLoVjZ86h",
        "outputId": "e4e201ce-6e3d-4f00-b8a1-550cd18bec35"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Request failed: HTTP 404 Not Found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-a86517f14c12>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgpt4all\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPT4All\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT4All\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ggml-model.bin\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mapp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFastAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gpt4all/gpt4all.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name, model_path, model_type, allow_download, n_threads, device, n_ctx, ngl, verbose)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;31m# Retrieve model and download if allowed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConfigType\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLLModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevice_init\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gpt4all/gpt4all.py\u001b[0m in \u001b[0;36mretrieve_model\u001b[0;34m(cls, model_name, model_path, allow_download, verbose)\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0;31m# If model file does not exist, download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0mfilesize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"filesize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             config[\"path\"] = str(cls.download_model(\n\u001b[0m\u001b[1;32m    342\u001b[0m                 \u001b[0mmodel_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"url\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m                 \u001b[0mexpected_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilesize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilesize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_md5\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"md5sum\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gpt4all/gpt4all.py\u001b[0m in \u001b[0;36mdownload_model\u001b[0;34m(model_filename, model_path, verbose, url, expected_size, expected_md5)\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0mtotal_size_in_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"content-length\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gpt4all/gpt4all.py\u001b[0m in \u001b[0;36mmake_request\u001b[0;34m(offset)\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m206\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Request failed: HTTP {response.status_code} {response.reason}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m206\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Content-Range'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Connection was interrupted and server does not support range requests'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Request failed: HTTP 404 Not Found"
          ]
        }
      ],
      "source": [
        "  # server.py (FastAPI)\n",
        "  from fastapi import FastAPI, HTTPException\n",
        "  from pydantic import BaseModel\n",
        "  from gpt4all import GPT4All\n",
        "\n",
        "  model = GPT4All(\"ggml-model.bin\")\n",
        "\n",
        "  app = FastAPI()\n",
        "\n",
        "  class ChatRequest(BaseModel):\n",
        "      message: str\n",
        "      api_key: str\n",
        "\n",
        "  @app.post(\"/chat\")\n",
        "  async def chat(request: ChatRequest):\n",
        "      if not validate_api_key(request.api_key):\n",
        "          raise HTTPException(status_code=403, detail=\"Invalid API Key\")\n",
        "      response = model.generate(request.message)\n",
        "      return {\"response\": response}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt4all import GPT4All\n",
        "\n",
        "model = GPT4All(\"/content/Llama-3.2-1B-Instruct-IQ3_M.gguf\", ngl=-1) # device='amd', device='intel'\n",
        "output = model.generate(\"The capital of France is \", max_tokens=111)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwOsT_BIeQrf",
        "outputId": "6ef0607f-387d-4dbb-e567-a9c9c6cc1967"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Paris.\n",
            "The largest planet in our solar system is Jupiter.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qa-Lv-Gteq5j",
        "outputId": "b97b21c9-0592-49f7-b73a-68ff177d9ed6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-24 19:41:40--  https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 3.169.137.19, 3.169.137.111, 3.169.137.5, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.169.137.19|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/e3/d1/e3d1a1e0851fc7d0ac2fb38ba7fdbf065971c29e410826e3c21d2f7f4c7a67a5/14850c84ff9f06e9b51d505d64815d5cc0cea0257380353ac0b3d21b21f6e024?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Mistral-7B-Instruct-v0.3.Q4_K_M.gguf%3B+filename%3D%22Mistral-7B-Instruct-v0.3.Q4_K_M.gguf%22%3B&Expires=1740429700&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDQyOTcwMH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2UzL2QxL2UzZDFhMWUwODUxZmM3ZDBhYzJmYjM4YmE3ZmRiZjA2NTk3MWMyOWU0MTA4MjZlM2MyMWQyZjdmNGM3YTY3YTUvMTQ4NTBjODRmZjlmMDZlOWI1MWQ1MDVkNjQ4MTVkNWNjMGNlYTAyNTczODAzNTNhYzBiM2QyMWIyMWY2ZTAyND9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=QlqAOCPu6kYvWkgoGcj4eioZQWknyGD4zb22et7hLq6LKlKwy3Kvgs0cCIRjDp7HyBq4IwEnhVdPv7-vSP6TgV555PE894R-zynWDyN0fKbi6BwrigWiiloSBK5vhcEoa%7E7zy87xe2uoYeQLaR376L4H5-IOPtz3o7zfj-iN55UWilDvaO1XXAcJprzuyyaeABH7XZV1JgZYiEHTs8o3Mt4wARE4p1e-BYGEle-O8eiZa9-Wa7JPBYVzD08i8StkKRO5ZLHHW6Wsrbp1eiTmuahckRVa4jpevRLVxtRwIUTt9OHsJz3IZ69JemBbgQYUrcQWHcW89M8jEhJPJ0-6kw__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-02-24 19:41:40--  https://cdn-lfs-us-1.hf.co/repos/e3/d1/e3d1a1e0851fc7d0ac2fb38ba7fdbf065971c29e410826e3c21d2f7f4c7a67a5/14850c84ff9f06e9b51d505d64815d5cc0cea0257380353ac0b3d21b21f6e024?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Mistral-7B-Instruct-v0.3.Q4_K_M.gguf%3B+filename%3D%22Mistral-7B-Instruct-v0.3.Q4_K_M.gguf%22%3B&Expires=1740429700&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDQyOTcwMH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2UzL2QxL2UzZDFhMWUwODUxZmM3ZDBhYzJmYjM4YmE3ZmRiZjA2NTk3MWMyOWU0MTA4MjZlM2MyMWQyZjdmNGM3YTY3YTUvMTQ4NTBjODRmZjlmMDZlOWI1MWQ1MDVkNjQ4MTVkNWNjMGNlYTAyNTczODAzNTNhYzBiM2QyMWIyMWY2ZTAyND9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=QlqAOCPu6kYvWkgoGcj4eioZQWknyGD4zb22et7hLq6LKlKwy3Kvgs0cCIRjDp7HyBq4IwEnhVdPv7-vSP6TgV555PE894R-zynWDyN0fKbi6BwrigWiiloSBK5vhcEoa%7E7zy87xe2uoYeQLaR376L4H5-IOPtz3o7zfj-iN55UWilDvaO1XXAcJprzuyyaeABH7XZV1JgZYiEHTs8o3Mt4wARE4p1e-BYGEle-O8eiZa9-Wa7JPBYVzD08i8StkKRO5ZLHHW6Wsrbp1eiTmuahckRVa4jpevRLVxtRwIUTt9OHsJz3IZ69JemBbgQYUrcQWHcW89M8jEhJPJ0-6kw__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 3.169.36.31, 3.169.36.120, 3.169.36.38, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|3.169.36.31|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4372811936 (4.1G) [binary/octet-stream]\n",
            "Saving to: ‘Mistral-7B-Instruct-v0.3.Q4_K_M.gguf’\n",
            "\n",
            "Mistral-7B-Instruct 100%[===================>]   4.07G  34.4MB/s    in 2m 1s   \n",
            "\n",
            "2025-02-24 19:43:41 (34.5 MB/s) - ‘Mistral-7B-Instruct-v0.3.Q4_K_M.gguf’ saved [4372811936/4372811936]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/ibm-ai-platform/Bamba-9B/resolve/main/bamba-9b.gguf\n"
      ],
      "metadata": {
        "id": "GMMsu4kKfaVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt4all import GPT4All\n",
        "\n",
        "model = GPT4All(\"/content/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\", ngl=-1) # device='amd', device='intel'\n",
        "output = model.generate(\"make chatbot with gpt4all\", max_tokens=1511)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFad2-n4ezx9",
        "outputId": "4fe416f0-ac2c-4d68-a3c6-57b4525c362a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "To create a chatbot using GPT-4ALL, you'll need to follow these steps:\n",
            "\n",
            "1. Install the necessary dependencies:\n",
            "   - Python 3.x (preferably 3.8 or later)\n",
            "   - pip (Python package manager)\n",
            "   - transformers library by Hugging Face\n",
            "   - torch library for deep learning\n",
            "\n",
            "You can install these packages using the following command in your terminal/command prompt:\n",
            "\n",
            "```bash\n",
            "pip install torch transformers\n",
            "```\n",
            "\n",
            "2. Download GPT-4ALL model weights and configuration files from the official GitHub repository (https://github.com/AwesomeVM/GPT-4All). Extract the downloaded archive to a convenient location on your system.\n",
            "\n",
            "3. Create a Python script for your chatbot:\n",
            "\n",
            "```bash\n",
            "touch gpt_chatbot.py\n",
            "```\n",
            "\n",
            "Open `gpt_chatbot.py` in your favorite text editor and paste the following code as a starting point:\n",
            "\n",
            "```python\n",
            "import torch\n",
            "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
            "\n",
            "def load_model(model_name):\n",
            "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
            "    model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
            "    return tokenizer, model\n",
            "\n",
            "def preprocess_input(tokenizer, text):\n",
            "    inputs = tokenizer.encode_plus(text, add_special_tokens=True, max_length=1024, padding=\"max_length\", truncation=True)\n",
            "    input_ids = inputs[\"input_ids\"][0]\n",
            "    attention_mask = inputs[\"attention_mask\"][0]\n",
            "    return torch.tensor([input_ids]), torch.tensor([attention_mask])\n",
            "\n",
            "def generate(model, tokenizer, prompt):\n",
            "    inputs = preprocess_input(tokenizer, prompt)\n",
            "    model.eval()\n",
            "    with torch.no_grad():\n",
            "        outputs = model(*inputs)\n",
            "        logits = outputs[0][-1]  # The last token is the predicted next token\n",
            "        probs = torch.softmax(logits, dim=-1).tolist()[0]\n",
            "        next_word_index = probs.index(max(probs))\n",
            "    return next_word_index\n",
            "\n",
            "def chatbot_response(model_name, prompt):\n",
            "    tokenizer, model = load_model(model_name)\n",
            "    response = \"\"\n",
            "    while True:\n",
            "        user_input = input(prompt + \"You: \")\n",
            "        if not user_input:\n",
            "            break\n",
            "        generated_index = generate(model, tokenizer, response + \" \" + user_input)\n",
            "        word = tokenizer.decode(generated_index)\n",
            "        response += \" Assistant: \" + word + \"\\n\"\n",
            "        print(\"Assistant:\", word)\n",
            "    return response\n",
            "```\n",
            "\n",
            "4. Replace the `model_name` variable in the `chatbot_response()` function with the path to your downloaded GPT-4ALL model weights and configuration files (e.g., \"gpt2\").\n",
            "\n",
            "5. Run your chatbot:\n",
            "\n",
            "```bash\n",
            "python gpt_chatbot.py\n",
            "```\n",
            "\n",
            "Now you can interact with your custom chatbot! Type a message, press Enter, and watch the bot respond using GPT-4ALL.\n"
          ]
        }
      ]
    }
  ]
}